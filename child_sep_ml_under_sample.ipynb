{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame with low_memory=False to avoid DtypeWarning\n",
    "data = pd.read_csv('msna_data.csv', low_memory=False)\n",
    "\n",
    "# Explore the data\n",
    "print(\"\\nInformation about the DataFrame:\")\n",
    "print(data.info())\n",
    "\n",
    "print(\"\\nSummary statistics of the DataFrame:\")\n",
    "print(data.describe(include='all'))\n",
    "\n",
    "# Display the data types of each column\n",
    "print(\"\\nData types of each column:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Cleaning\n",
    "\n",
    "The data cleaning process begins by removing rows with missing values in the target column, `child_sep_severity`. Next, only the relevant variables from the `vars_analysis.csv` file are retained for further analysis. Columns with more than 10% missing values are dropped to ensure data quality. Additionally, columns with zero variance, which provide no useful information, are removed. Variables ending in '_NA' without corresponding non-NA versions are also discarded to avoid redundancy. Missing data patterns are visualized both before and after these cleaning steps. Finally, the cleaned dataset is reviewed to ensure all essential variables remain, with any dropped variables from `vars_analysis` reported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "\n",
    "# Define function to display and drop columns based on a condition\n",
    "def drop_columns_with_condition(df, condition, description):\n",
    "    columns_to_drop = df.columns[condition].tolist()\n",
    "    if columns_to_drop:\n",
    "        print(f\"\\nDropped columns {description}:\")\n",
    "        print(columns_to_drop)\n",
    "        return df.drop(columns=columns_to_drop)\n",
    "    return df\n",
    "\n",
    "# Filter out rows with missing values in the 'child_sep_severity' column\n",
    "data_filtered = data.dropna(subset=['child_sep_severity'])\n",
    "print(\"\\nShape of the filtered DataFrame, without missing values for the response variable:\", data_filtered.shape)\n",
    "\n",
    "# Load variables to consider for the model\n",
    "vars_analysis = pd.read_csv('vars_analysis.csv')['variable'].tolist()\n",
    "\n",
    "# Select columns that are relevant for analysis\n",
    "data_select = data_filtered[vars_analysis].copy()\n",
    "print(\"\\nDataFrame after keeping variables in vars_analysis:\")\n",
    "data_select.info()\n",
    "\n",
    "# Identify columns with more than 20% missing values\n",
    "missing_percentage = data_select.isnull().mean() * 100\n",
    "columns_with_many_missing = missing_percentage > 20\n",
    "\n",
    "# Drop columns with more than 20% missing values\n",
    "data_select = drop_columns_with_condition(data_select, columns_with_many_missing, \"with more than 20% missing values\")\n",
    "\n",
    "# Drop columns with zero variance (only one unique value)\n",
    "zero_variance_condition = data_select.nunique() <= 1\n",
    "data_select = drop_columns_with_condition(data_select, zero_variance_condition, \"with zero variance\")\n",
    "\n",
    "# Identify and drop variables ending with '_NA' that have no other variable with the same stem\n",
    "na_columns = [col for col in data_select.columns if col.endswith('_NA')]\n",
    "stems = {col[:-3] for col in na_columns}\n",
    "na_suffix_condition = [col for col in na_columns if not any(other_col.startswith(col[:-3]) and other_col != col for other_col in data_select.columns)]\n",
    "data_select = drop_columns_with_condition(data_select, data_select.columns.isin(na_suffix_condition), \"'_NA' with no matching stem variable\")\n",
    "\n",
    "# Assign the final cleaned data to data_select_na\n",
    "data_select_na = data_select\n",
    "\n",
    "# Visualize missing data matrix before dropping columns\n",
    "msno.matrix(data_filtered)\n",
    "plt.show()\n",
    "\n",
    "# Visualize missing data matrix after dropping columns\n",
    "msno.matrix(data_select_na)\n",
    "plt.show()\n",
    "\n",
    "# Display final DataFrame info\n",
    "print(\"\\nFinal DataFrame after dropping unnecessary columns:\")\n",
    "data_select_na.info()\n",
    "\n",
    "# Check and display which variables from vars_analysis are no longer in the data\n",
    "missing_vars = [var for var in vars_analysis if var not in data_select_na.columns]\n",
    "print(\"\\nVariables from vars_analysis no longer in the data:\", missing_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Change Encoding of Ordinal Variables\n",
    "\n",
    "The script converts one-hot encoded ordinal variables into a more appropriate label-encoded format. It begins by loading a mapping of old one-hot labels and new ordinal labels from `new_labels.csv`, filtering for variables present in the dataset. The script then replaces one-hot encoded values with the corresponding ordinal labels and converts the recoded variables to integer type, ensuring proper handling of missing values. It verifies that all required variables have been recoded and displays the updated data along with the data types of the recoded variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new_labels CSV file and filter for variables present in the data\n",
    "new_labels = pd.read_csv('new_labels.csv', usecols=['variable', 'label_old', 'label_new'])\n",
    "new_labels = new_labels[new_labels['variable'].isin(data_select_na.columns)]\n",
    "\n",
    "print(\"\\nLoaded new_labels DataFrame:\")\n",
    "print(new_labels.head())\n",
    "\n",
    "# Create a copy of data_select_na to apply changes\n",
    "data_ord = data_select_na.copy()\n",
    "\n",
    "# Apply the label mapping to the data_ord DataFrame\n",
    "for _, row in new_labels.iterrows():\n",
    "    variable = row['variable']\n",
    "    label_old = row['label_old']\n",
    "    label_new = int(row['label_new'])  # Convert label_new to integer\n",
    "    \n",
    "    if variable in data_ord.columns:\n",
    "        data_ord[variable] = data_ord[variable].replace(label_old, label_new)\n",
    "\n",
    "# Define recoded_variables\n",
    "recoded_variables = new_labels['variable'].unique()\n",
    "\n",
    "# Convert the recoded variables to integer type\n",
    "for variable in recoded_variables:\n",
    "    if variable in data_ord.columns:\n",
    "        data_ord[variable] = data_ord[variable].fillna(pd.NA).astype('Int64')  # Fill NaNs with pd.NA before converting\n",
    "\n",
    "# Check if recoded_variables are in data_ord\n",
    "variables_in_data_ord = all(variable in data_ord.columns for variable in recoded_variables)\n",
    "print(f\"All recoded variables are in data_ord: {variables_in_data_ord}\")\n",
    "\n",
    "# Identify which variables are not in data_ord\n",
    "missing_variables = [variable for variable in recoded_variables if variable not in data_ord.columns]\n",
    "print(f\"Variables not in data_ord: {missing_variables}\")\n",
    "\n",
    "# Display the first few rows of the recoded variables in the data_ord DataFrame\n",
    "print(\"\\nRecoded variables in data_ord DataFrame:\")\n",
    "print(data_ord[recoded_variables].head())\n",
    "\n",
    "# Display the variable types of the recoded variables\n",
    "print(\"\\nVariable types of recoded variables:\")\n",
    "print(data_ord[recoded_variables].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 KNN Imputation of Remaining Missing Values\n",
    "\n",
    "Next, we use **K-Nearest Neighbors (KNN) imputation** to handle missing values across all columns in the dataset. It first applies the `KNNImputer` to estimate and replace missing values based on the nearest neighbors. After imputation, the missing data is visualized, and the updated dataset with the imputed values is displayed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to apply K-Nearest Neighbors Imputation to all columns\n",
    "def knn_impute_all_columns(df, n_neighbors=5):\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "    return df_imputed\n",
    "\n",
    "# Apply KNN imputation to all columns\n",
    "data_na_imp = knn_impute_all_columns(data_ord.copy())\n",
    "\n",
    "# Visualize missing data matrix\n",
    "msno.matrix(data_na_imp)\n",
    "plt.show()\n",
    "\n",
    "# Display the first few rows of the new dataframe with imputed values\n",
    "print(\"\\nDataFrame with imputed values in all columns:\")\n",
    "print(data_na_imp.head())\n",
    "\n",
    "# Display the first few rows of the new dataframe with imputed values\n",
    "print(\"\\nDataFrame with imputed values in the first column:\")\n",
    "print(data_na_imp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Standardization\n",
    "\n",
    "This process standardizes the **numerical variables** in the dataset while preserving categorical and dummy variables. After identifying variable types from `vars_analysis.csv`, only numerical variables are standardized using `StandardScaler` to ensure a mean of 0 and standard deviation of 1. Categorical variables are added back without standardization. The script displays summary statistics before and after standardization and visualizes the distribution of the `num_children` variable with histograms to illustrate the effect of standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the vars_analysis.csv to identify variable types\n",
    "vars_analysis = pd.read_csv('vars_analysis.csv', usecols=['variable', 'type'])\n",
    "dummy_variables = vars_analysis[vars_analysis['type'] == 'dummy']['variable'].tolist()\n",
    "categorical_variables = vars_analysis[vars_analysis['type'] == 'ordinal']['variable'].tolist()\n",
    "numerical_variables = vars_analysis[vars_analysis['type'] == 'numerical']['variable'].tolist()\n",
    "\n",
    "# Filter variables to include only those present in data_dum\n",
    "dummy_variables = [var for var in dummy_variables if var in data_na_imp.columns]\n",
    "categorical_variables = [var for var in categorical_variables if var in data_na_imp.columns]\n",
    "numerical_variables = [var for var in numerical_variables if var in data_na_imp.columns]\n",
    "\n",
    "# Standardize the numerical variables (excluding dummy variables and categorical variables)\n",
    "scaler = StandardScaler()\n",
    "data_stand = data_na_imp.copy()\n",
    "data_stand[numerical_variables] = scaler.fit_transform(data_stand[numerical_variables])\n",
    "\n",
    "# Add back the categorical variables without standardizing them\n",
    "common_categorical_vars = list(set(categorical_variables).intersection(data_na_imp.columns))\n",
    "data_stand[common_categorical_vars] = data_na_imp[common_categorical_vars]\n",
    "\n",
    "# Display summary statistics before standardization\n",
    "print(\"\\nSummary statistics before standardization:\")\n",
    "print(data_na_imp.describe())\n",
    "\n",
    "# Display summary statistics after standardization\n",
    "print(\"\\nSummary statistics after standardization:\")\n",
    "print(data_stand.describe())\n",
    "\n",
    "# Histogram for hoh_age before standardization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data_na_imp['num_children'], bins=30, alpha=0.7, color='blue')\n",
    "plt.title('num_children before standardization')\n",
    "plt.xlabel('num_children')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for num_children after standardization\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(data_stand['num_children'], bins=30, alpha=0.7, color='green')\n",
    "plt.title('num_children after standardization')\n",
    "plt.xlabel('num_children')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection\n",
    "\n",
    "This process performs **feature selection** using various methods to identify the most important features for predicting the target variable. The dataset is first prepared by ensuring all features are numeric, free of missing values, and standardized. Several feature selection methods are applied:\n",
    "1. **Recursive Feature Elimination (RFE)**: Selects features by recursively removing the least important ones based on a logistic regression model.\n",
    "2. **Random Forest Feature Importance**: Identifies the top features based on the importance scores from a Random Forest model.\n",
    "3. **Elastic Net Regularization**: Selects features based on the non-zero coefficients from an Elastic Net model.\n",
    "4. **Boruta Algorithm**: A wrapper method using Random Forest to identify all relevant features.\n",
    "5. **Mutual Information**: Measures the dependency between each feature and the target.\n",
    "\n",
    "Finally, the selected features from all methods are combined, and the dataset is filtered to retain only those features, preparing it for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from boruta import BorutaPy\n",
    "import shap\n",
    "\n",
    "# Assuming data_stand is already defined and loaded\n",
    "\n",
    "# Create a copy of data_stand for feature selection\n",
    "data_fs = data_stand.copy()\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_fs.drop(columns=['child_sep_severity_family_reun_required'])\n",
    "y = data_fs['child_sep_severity_family_reun_required']\n",
    "\n",
    "### Convert All Variables to Float ###\n",
    "\n",
    "# Force all features to float64\n",
    "X = X.astype(np.float64)\n",
    "y = y.astype(np.float64)\n",
    "\n",
    "### Utility Functions for Checks ###\n",
    "\n",
    "# Check if all columns are numeric\n",
    "def check_numeric(df):\n",
    "    if df.select_dtypes(include=[np.number]).shape[1] != df.shape[1]:\n",
    "        raise ValueError(\"All features must be numeric. Please convert categorical variables to numeric format.\")\n",
    "\n",
    "# Check for missing values\n",
    "def check_no_missing(df):\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        raise ValueError(\"Input data contains missing values. Please handle missing data before applying these methods.\")\n",
    "\n",
    "# Ensure target variable is binary or multiclass\n",
    "def check_target(y):\n",
    "    if not pd.api.types.is_numeric_dtype(y):\n",
    "        raise ValueError(\"Target variable 'y' must be numeric.\")\n",
    "    if len(y.unique()) < 2:\n",
    "        raise ValueError(\"Target variable 'y' must have at least two classes for classification.\")\n",
    "\n",
    "# Check if data is standardized and print variables that are not\n",
    "def check_standardized(df, mean_threshold=0.01, std_threshold=0.01):\n",
    "    # Calculate the absolute difference of means from 0 and standard deviations from 1\n",
    "    mean_diff = df.mean().abs()  # Difference from 0 for means\n",
    "    std_diff = (df.std() - 1).abs()  # Difference from 1 for standard deviations\n",
    "\n",
    "    # Identify columns that are not standardized\n",
    "    non_standardized_vars = df.columns[(mean_diff > mean_threshold) | (std_diff > std_threshold)].tolist()\n",
    "    \n",
    "    # Print warning only if there are non-standardized variables\n",
    "    if non_standardized_vars:\n",
    "        print(\"Warning: The following variables do not appear to be standardized (mean ≈ 0, std ≈ 1):\")\n",
    "        print(non_standardized_vars)\n",
    "\n",
    "### 1. Ensure Input Data is Correct for Each Method ###\n",
    "\n",
    "# Check target variable\n",
    "check_target(y)\n",
    "\n",
    "# Check if features are numeric and have no missing values\n",
    "check_numeric(X)\n",
    "check_no_missing(X)\n",
    "\n",
    "### 2. Check Standardization for Methods Sensitive to Scale ###\n",
    "check_standardized(X)\n",
    "\n",
    "### 3. Recursive Feature Elimination (RFE) ###\n",
    "try:\n",
    "    print(\"Running RFE...\")\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    rfe = RFE(model, n_features_to_select=10)\n",
    "    X_rfe = rfe.fit_transform(X, y)\n",
    "    print(f\"Features after RFE: {X_rfe.shape[1]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running RFE: {e}\")\n",
    "\n",
    "### 4. Feature Importance from Tree-Based Models ###\n",
    "try:\n",
    "    print(\"Running Random Forest...\")\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_features_rf = X.columns[indices[:10]]\n",
    "    print(f\"Top features according to Random Forest: {top_features_rf}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running Random Forest: {e}\")\n",
    "\n",
    "### 5. Elastic Net Regularization ###\n",
    "try:\n",
    "    print(\"Running Elastic Net...\")\n",
    "    elastic_net = ElasticNetCV(cv=5, random_state=42)\n",
    "    elastic_net.fit(X, y)\n",
    "    enet_coef = pd.Series(elastic_net.coef_, index=X.columns)\n",
    "    top_features_enet = enet_coef[enet_coef != 0].index\n",
    "    print(f\"Top features according to Elastic Net: {top_features_enet}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running Elastic Net: {e}\")\n",
    "\n",
    "### 6. Boruta Algorithm ###\n",
    "try:\n",
    "    print(\"Running Boruta...\")\n",
    "    boruta_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=42)\n",
    "    boruta_selector.fit(X.values, y)\n",
    "    boruta_features = X.columns[boruta_selector.support_].tolist()\n",
    "    print(f\"Top features according to Boruta: {boruta_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running Boruta: {e}\")\n",
    "\n",
    "### 7. Mutual Information ###\n",
    "try:\n",
    "    print(\"Calculating Mutual Information...\")\n",
    "    mi = mutual_info_classif(X, y)\n",
    "    mi_features = X.columns[np.argsort(mi)[-10:]].tolist()\n",
    "    print(f\"Top features according to Mutual Information: {mi_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating Mutual Information: {e}\")\n",
    "\n",
    "### Combine Results for the Most Relevant Features ###\n",
    "try:\n",
    "    combined_features = set(top_features_rf) | set(top_features_enet) | set(boruta_features) | set(mi_features)\n",
    "    print(f\"Combined selected features: {combined_features}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error combining selected features: {e}\")\n",
    "\n",
    "### Drop Unselected Features ###\n",
    "\n",
    "# Find the intersection of selected features from all methods\n",
    "selected_features = list(combined_features)  # Convert set to list for indexing\n",
    "\n",
    "# Filter the original dataset to keep only the selected features\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "# Print the shape of the new dataset to confirm feature selection\n",
    "print(f\"Shape of data after dropping unselected features: {X_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Splitting\n",
    "\n",
    "Next, we split the dataset into **training**, **validation**, and **test** sets while maintaining the class distribution due to the highly imbalanced nature of the target variable.\n",
    "\n",
    "1. **Initial Split**: The dataset is first divided into an 80% **train + validation** set and a 20% **test set**.\n",
    "2. **Train/Validation Split**: The train + validation set is further split into a 75% **training set** and a 25% **validation set**.\n",
    "   - This results in:\n",
    "     - **Training Set**: 60% of the original data\n",
    "     - **Validation Set**: 20% of the original data\n",
    "     - **Test Set**: 20% of the original data\n",
    "3. **Class Distribution**: After splitting, the number of samples and the class distribution (for both classes 0 and 1) are printed for each dataset to ensure the splits maintain the correct proportions of each class.\n",
    "\n",
    "This ensures proper stratified splitting of the dataset for training, hyperparameter tuning, and final testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X is your feature set and y is your highly imbalanced response variable\n",
    "# First, create a train + validation set and a test set (e.g., 80% train/validation, 20% test)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_selected, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Then, split the train + validation set into the actual training set and validation set (e.g., 75% train, 25% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)\n",
    "\n",
    "# Result:\n",
    "# - X_train, y_train: Training data (60% of the original dataset)\n",
    "# - X_val, y_val: Validation data (20% of the original dataset)\n",
    "# - X_test, y_test: Test data (20% of the original dataset)\n",
    "\n",
    "# Print the structure of the resulting splits\n",
    "print(\"Training Set\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Samples: {X_train.shape[0]}\")\n",
    "print(f\"  Class 0: {np.sum(y_train == 0)}\")\n",
    "print(f\"  Class 1: {np.sum(y_train == 1)}\")\n",
    "print(\"\\nValidation Set\")\n",
    "print(f\"  Features: {X_val.shape[1]}\")\n",
    "print(f\"  Samples: {X_val.shape[0]}\")\n",
    "print(f\"  Class 0: {np.sum(y_val == 0)}\")\n",
    "print(f\"  Class 1: {np.sum(y_val == 1)}\")\n",
    "print(\"\\nTest Set\")\n",
    "print(f\"  Features: {X_test.shape[1]}\")\n",
    "print(f\"  Samples: {X_test.shape[0]}\")\n",
    "print(f\"  Class 0: {np.sum(y_test == 0)}\")\n",
    "print(f\"  Class 1: {np.sum(y_test == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Balancing\n",
    "\n",
    "This script applies **SMOTEENN (Synthetic Minority Over-sampling Technique + Edited Nearest Neighbors)** to balance the training data and ensure feature consistency between the training, validation, and test sets.\n",
    "\n",
    "1. **SMOTEENN**: Balances the training data by oversampling the minority class and removing noisy samples from the majority class, resulting in `X_train_resampled` and `y_train_resampled`.\n",
    "2. **Column Check (Before Reindexing)**: The script checks for any missing or extra columns in `X_val` and `X_test` compared to `X_train_resampled`, identifying any discrepancies in feature alignment.\n",
    "3. **Reindexing**: Validation and test sets are reindexed to ensure they have the same columns as `X_train_resampled`, with missing columns filled with zeros.\n",
    "4. **Column Check (After Reindexing)**: A second check confirms that the feature columns in `X_val` and `X_test` now match the resampled training set.\n",
    "5. **Class Distribution**: The script displays the original and resampled class distributions, illustrating the effect of SMOTEENN on balancing the dataset.\n",
    "\n",
    "This step ensures that the resampled training data is balanced and that the validation and test sets are aligned with the training data for consistent model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "\n",
    "# Apply SMOTEENN to balance the training data\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check for column mismatches before reindexing\n",
    "print(\"Checking for column mismatches before reindexing...\")\n",
    "\n",
    "# Find columns missing in X_val or X_test\n",
    "missing_in_val = set(X_train_resampled.columns) - set(X_val.columns)\n",
    "missing_in_test = set(X_train_resampled.columns) - set(X_test.columns)\n",
    "\n",
    "# Find extra columns in X_val or X_test\n",
    "extra_in_val = set(X_val.columns) - set(X_train_resampled.columns)\n",
    "extra_in_test = set(X_test.columns) - set(X_train_resampled.columns)\n",
    "\n",
    "print(f\"Columns missing in X_val: {missing_in_val}\")\n",
    "print(f\"Columns missing in X_test: {missing_in_test}\")\n",
    "print(f\"Extra columns in X_val: {extra_in_val}\")\n",
    "print(f\"Extra columns in X_test: {extra_in_test}\")\n",
    "\n",
    "# Reindex the validation and test sets to match the training set\n",
    "X_val = X_val.reindex(columns=X_train_resampled.columns, fill_value=0)\n",
    "X_test = X_test.reindex(columns=X_train_resampled.columns, fill_value=0)\n",
    "\n",
    "# Check again for column mismatches after reindexing\n",
    "print(\"\\nChecking for column mismatches after reindexing...\")\n",
    "\n",
    "# Find columns missing in X_val or X_test (there should be none now)\n",
    "missing_in_val_after = set(X_train_resampled.columns) - set(X_val.columns)\n",
    "missing_in_test_after = set(X_train_resampled.columns) - set(X_test.columns)\n",
    "\n",
    "# Find extra columns in X_val or X_test (there should be none now)\n",
    "extra_in_val_after = set(X_val.columns) - set(X_train_resampled.columns)\n",
    "extra_in_test_after = set(X_test.columns) - set(X_train_resampled.columns)\n",
    "\n",
    "print(f\"Columns missing in X_val after reindexing: {missing_in_val_after}\")\n",
    "print(f\"Columns missing in X_test after reindexing: {missing_in_test_after}\")\n",
    "print(f\"Extra columns in X_val after reindexing: {extra_in_val_after}\")\n",
    "print(f\"Extra columns in X_test after reindexing: {extra_in_test_after}\")\n",
    "\n",
    "# Print class distribution after resampling\n",
    "print(f\"\\nOriginal class distribution: {Counter(y_train)}\")\n",
    "print(f\"Resampled class distribution: {Counter(y_train_resampled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Selection\n",
    "\n",
    "This process uses **GridSearchCV** to tune hyperparameters for three models—**Random Forest**, **Logistic Regression**, and **XGBoost**—while addressing class imbalance. These models are chosen because:\n",
    "- **Random Forest** is robust to overfitting and can handle non-linear relationships, making it effective for imbalanced data.\n",
    "- **Logistic Regression** provides a simple, interpretable model and can be adjusted with class weighting to handle imbalances.\n",
    "- **XGBoost** is a powerful boosting algorithm that excels at complex datasets and imbalanced data with the right hyperparameters.\n",
    "\n",
    "The models are configured with:\n",
    "- **Class Weighting** for Random Forest and Logistic Regression (`class_weight='balanced'`) to give more weight to the minority class.\n",
    "- **Scale Positive Weight** for XGBoost (`scale_pos_weight`) to adjust the weight of the minority class.\n",
    "\n",
    "The models are evaluated using the **F1-Score** as the scoring metric in GridSearchCV to focus on optimizing performance for the minority class by balancing precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define candidate models with class weighting for imbalance handling\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    'XGBoost': XGBClassifier(scale_pos_weight=len(y_train_resampled[y_train_resampled == 0]) / len(y_train_resampled[y_train_resampled == 1]), random_state=42)\n",
    "}\n",
    "\n",
    "# Define hyperparameters for GridSearch\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best model and hyperparameters with F1-Score as the evaluation metric\n",
    "best_models = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    # Switch from roc_auc to f1 to better handle minority class performance\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=5, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best params for {model_name}: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "We now evaluate the performance of multiple trained models on a **validation set** to understand how well they generalize to unseen data.\n",
    "\n",
    "For each model:\n",
    "- We use **ROC-AUC** to measure its ability to distinguish between positive and negative classes, which is crucial for imbalanced datasets.\n",
    "- We calculate the **F1-Score** to assess the balance between precision and recall, providing insight into the model’s overall classification effectiveness.\n",
    "- A **classification report** is generated to give detailed metrics, including precision, recall, and F1-score for both the majority and minority classes.\n",
    "\n",
    "By comparing these metrics across models, we aim to select the one that performs best, generalizes well, and effectively handles class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "\n",
    "for model_name, model in best_models.items():\n",
    "    print(f\"\\nEvaluating {model_name} on validation set:\")\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_val, y_val_prob)}\")\n",
    "    print(f\"F1-Score: {f1_score(y_val, y_val_pred)}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Retraining\n",
    "\n",
    "Once the best model is selected based on its performance on the validation set, we retrain it using the **full resampled training dataset** to take advantage of all the available data.\n",
    "\n",
    "In this step:\n",
    "- We identify the best model by comparing their **ROC-AUC** scores from the validation phase.\n",
    "- The chosen model is then retrained using the entire **balanced training set**, which was resampled using techniques like **SMOTE + ENN** to address class imbalance.\n",
    "- By using all the available training data, we ensure that the model has the maximum amount of information for learning, leading to better generalization during final testing.\n",
    "\n",
    "This step prepares the model for its final evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Select the best model based on F1-Score for the minority class\n",
    "best_model_name = max(best_models, key=lambda name: f1_score(y_val, best_models[name].predict(X_val)))\n",
    "\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "print(f\"Retraining the best model: {best_model_name}...\")\n",
    "best_model.fit(X_train_resampled, y_train_resampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation on Test Set\n",
    "\n",
    "After retraining the best model on the full resampled training data, we evaluate its performance on the **test set**, which was kept aside during the initial data splitting.\n",
    "\n",
    "In this step:\n",
    "- We use the **test set** (previously unseen by the model) to generate predictions and assess how well the model generalizes to completely new data.\n",
    "- **ROC-AUC** is computed to measure how well the model distinguishes between positive and negative classes in this final evaluation.\n",
    "- The **F1-Score** is calculated to evaluate the balance between precision and recall, which remains important given the imbalanced nature of the dataset.\n",
    "- A **classification report** provides a detailed breakdown of precision, recall, and F1-scores for both classes, allowing us to fully understand the model's performance on real-world data.\n",
    "\n",
    "This final evaluation gives an unbiased estimate of the model's performance before deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the best model\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Ensure that predict_proba works for XGBClassifier and get the probabilities for the positive class\n",
    "y_test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the performance of the best model on the test set\n",
    "print(f\"\\nTest Set Performance for {best_model_name}:\")\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_test_prob)}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_test_pred)}\")\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of XGBoost Test Set Performance:\n",
    "\n",
    "#### 1. **ROC-AUC Score: 0.777**\n",
    "This score indicates a **moderate ability** to distinguish between the two classes (0 and 1). With a score of 0.777, the model is somewhat good at ranking class 1 instances higher than class 0, but it's not particularly strong.\n",
    "\n",
    "#### 2. **F1-Score: 0.070**\n",
    "The **F1-Score** is very low, showing that the model struggles to balance precision and recall for the minority class (class 1). This suggests poor performance in detecting true positives while avoiding false positives.\n",
    "\n",
    "#### 3. **Classification Report**:\n",
    "- **Class 0 (Majority Class)**:\n",
    "  - **Precision: 1.00**: The model almost perfectly identifies class 0, with nearly all predictions of class 0 being correct.\n",
    "  - **Recall: 0.98**: It captures 98% of all class 0 instances, missing only a small percentage.\n",
    "  - **F1-Score: 0.99**: High precision and recall lead to a very strong F1-score for class 0.\n",
    "\n",
    "- **Class 1 (Minority Class)**:\n",
    "  - **Precision: 0.04**: Only 4% of the predictions for class 1 are correct, meaning most of them are false positives.\n",
    "  - **Recall: 0.25**: The model captures 25% of actual class 1 instances, missing 75%.\n",
    "  - **F1-Score: 0.07**: Low precision and recall result in a poor F1-score for class 1, indicating significant difficulty in identifying the minority class.\n",
    "\n",
    "#### 4. **Accuracy: 0.97**\n",
    "While the overall accuracy is 97%, this is **misleading** due to the class imbalance. The high accuracy is mainly driven by the model's performance on class 0, not its ability to detect class 1.\n",
    "\n",
    "#### 5. **Macro Average**:\n",
    "- **Precision: 0.52**, **Recall: 0.61**, **F1-Score: 0.53**: These averages show that while the model performs well on class 0, the poor performance on class 1 lowers the overall averages.\n",
    "\n",
    "#### 6. **Weighted Average**:\n",
    "- The **weighted averages** reflect the dominance of class 0 in the dataset. The high weighted precision (0.99), recall (0.97), and F1-score (0.98) are largely influenced by the model's success in predicting class 0, masking the poor performance on class 1.\n",
    "\n",
    "#### Conclusion:\n",
    "- **Strong performance on Class 0**: The model excels at predicting class 0, with near-perfect precision and recall.\n",
    "- **Weak performance on Class 1**: It struggles significantly with class 1, having very low precision (0.04) and recall (0.25), meaning most class 1 predictions are incorrect.\n",
    "- **Overall**: Despite the high accuracy and ROC-AUC score, the model is ineffective at identifying the minority class. Further improvements, such\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
